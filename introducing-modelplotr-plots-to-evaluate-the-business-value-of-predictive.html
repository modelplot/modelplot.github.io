<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
  <head>
    <title>'Introducing modelplotr: Plots to evaluate the business value of predictive - intro modelplot</title>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <link href="//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css" rel="stylesheet" />
    <!--<link href="//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap-theme.min.css" rel="stylesheet" />-->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet" />
    <link href="https://modelplot.github.io/theme/style.css" rel="stylesheet" />
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link href="https://modelplot.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="intro modelplot Full Atom Feed" />
    <link href="https://modelplot.github.io/feeds/misc.atom.xml" type="application/atom+xml" rel="alternate" title="intro modelplot Categories Atom Feed" />
  </head>
  <body id="index" class="archive">
    <!--[if lt IE 7]>
        <p class="browsehappy">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</p>
    <![endif]-->
    <nav class="navbar navbar-default" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="https://modelplot.github.io">intro modelplot</a>
        </div>
        <div class="collapse navbar-collapse navbar-right">
          <ul class="nav navbar-nav">
            <li><a href="https://modelplot.github.io/tags.html">tags</a></li>
          </ul>
        </div>
        <!-- /.navbar-collapse -->
      </div>
    </nav>
    <div class="container">
    <section id="content" class="article content">
      <header>
        <h2 class="entry-title">
          'Introducing modelplotr: Plots to evaluate the business value of predictive
        </h2>
        
        <div class="text-muted">wo 25 juli 2018</div>
      </header>
<!-- .entry-content -->
      <div class="entry-content">
        <p>models'
author: "Jurriaan Nagelkerke and Pieter Marcus"
date: '2018-08-17'
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
tags:
- R
- predictive modeling
categories:
- R
- modelplotr
- modelplot
- modelplotpy</p>
<hr />
<h2>Why ROC curves are a bad idea to explain your model to business people</h2>
<h3>Summary</h3>
<p>In this blog we explain four most valuable evaluation charts to assess the business value of a predictive model. Since these visualisations are not included in most popular model building packages or modules in R and Python, we show how you can easily create these plots for your own predictive models with our modelplotr r package. This will help you to explain your model's business value in laymans terms to non-techies. Do you prefer python? (Use <a href="https://www.github.com/modelplot/modelplotpy">modelplotpy</a> and read our intro to that module!)</p>
<h3>Intro</h3>
<p><img alt="''" src="img/cartoonrocplot.jpg" /></p>
<blockquote>
<p>‘...And as we can see clearly on this ROC plot, the sensitivity of the model at the value of 0.2 on one minus the specificity is quite high! Right?…’. </p>
</blockquote>
<p>If your fellow business colleagues didn’t already wander away during your presentation about your fantastic predictive model, it will definately push them over the edge when you start talking like this. Why? Because the ROC curve is not easy to quickly explain and also difficult to translate into answers on the business questions your spectators have. And these business questions were the reason you’ve built a model in the first place!   </p>
<p>What business questions? In most use cases, we build predictive models to select the best records in a dataset, which can be customers, leads, items, events... For instance: You want to know which of your active customers have the highest probability to churn; you need to select those prospects that are most likely to respond to an offer; you have to identify transactions that have a high risk to be fraudulent. During your preso, your audience is therefore mainly focused on answering questions like <em>Does your model enable us to our target audience? How much better are we, using your model? What will the expected response on our campaign be?</em>    </p>
<p>During our model building efforts, we should already be focused on verifying how well the model performs. Often, we do so by training the model parameters on a selection or subset of records and test the performance on a holdout set or external validation set. We look at a set of performance measures like the ROC curve and hit rates. Those plots and statistics are very helpful to check during model building and optimization whether your model is under- or overfitting and what set of parameters performs best on test data. However, these statistics are not that valuable in assessing the business value the model you developed. </p>
<p>One reason that the ROC curve is not that usefull in explaining the business value of your model, is because  it’s quite hard to explain the interpretation of ‘area under the curve’, ‘specificity’ or ‘sensitivity’ to business people. Another important reason that these statistics and plots are useless in your business meetings is that they don’t help in determining how to apply your predictive model: What percentage of records should we select based on the model? Should we select only the best 10% of cases? Or should we stop at 30%? Or go on until we have selected 70%?...  This is something you want to decide <em>together</em> with your business colleague, to best match the business plans and campaign targets they have to meet. The four plots we are about to introduce are in our view the best ones for that cause.</p>
<h3>Example: Predictive models from <em>mlr</em> on the <em>Bank Marketing Data Set</em></h3>
<p>Let's get to business! When we introduce the plots, we'll show you how to use them with a business example. This example is based on a publicly available dataset, called the Bank Marketing Data Set. It is one of the most popular datasets which is made available on the <a href="https://archive.ics.uci.edu/ml/index.php">UCI Machine Learning Repository</a>. The data set comes from a Portugese bank and deals with a frequently-posed marketing question: whether a customer did or did not acquire a term deposit, a financial product. There are 4 datasets available and the <strong>bank-additional-full.csv</strong> is the one we use. It contains the information of 41.188 customers and 21 columns of information. Since we want to show you how to build the plots, not how to build a perfect model, we'll use six of these columns in our example. Here’s a short description on the data we use:</p>
<ol>
<li><strong>y</strong>: has the client subscribed a term deposit?</li>
<li><strong>duration</strong>: last contact duration, in seconds (numeric).</li>
<li><strong>campaign</strong>: number of contacts performed during this campaign and for this client</li>
<li><strong>pdays</strong>: number of days that passed by after the client was last contacted from a previous campaign</li>
<li><strong>previous</strong>: number of contacts performed before this campaign and for this client (numeric)</li>
<li><strong>euribor3m</strong>: euribor 3 month rate</li>
</ol>
<p>Let's load the data and have a quick look at it:</p>
<div class="highlight"><pre><span></span><span class="c1"># download bank data, prepare and summarize</span>
zipname <span class="o">=</span> <span class="s">&#39;https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip&#39;</span>
csvname <span class="o">=</span> <span class="s">&#39;bank-additional/bank-additional.csv&#39;</span>
temp <span class="o">&lt;-</span> <span class="kp">tempfile</span><span class="p">()</span>
download.file<span class="p">(</span>zipname<span class="p">,</span>temp<span class="p">,</span> mode<span class="o">=</span><span class="s">&quot;wb&quot;</span><span class="p">)</span>
bank <span class="o">&lt;-</span> read.table<span class="p">(</span>unzip<span class="p">(</span>temp<span class="p">,</span>csvname<span class="p">),</span>sep<span class="o">=</span><span class="s">&quot;;&quot;</span><span class="p">,</span> stringsAsFactors<span class="o">=</span><span class="kc">FALSE</span><span class="p">,</span>header <span class="o">=</span> <span class="bp">T</span><span class="p">)</span>
<span class="kp">unlink</span><span class="p">(</span>temp<span class="p">)</span>
bank <span class="o">&lt;-</span> bank<span class="p">[,</span><span class="kt">c</span><span class="p">(</span><span class="s">&#39;y&#39;</span><span class="p">,</span><span class="s">&#39;duration&#39;</span><span class="p">,</span><span class="s">&#39;campaign&#39;</span><span class="p">,</span><span class="s">&#39;pdays&#39;</span><span class="p">,</span><span class="s">&#39;previous&#39;</span><span class="p">,</span><span class="s">&#39;euribor3m&#39;</span><span class="p">)]</span>

str<span class="p">(</span>bank<span class="p">)</span>
<span class="c1">## &#39;data.frame&#39;:    4119 obs. of  6 variables:</span>
<span class="c1">##  $ y        : chr  &quot;no&quot; &quot;no&quot; &quot;no&quot; &quot;no&quot; ...</span>
<span class="c1">##  $ duration : int  487 346 227 17 58 128 290 44 68 170 ...</span>
<span class="c1">##  $ campaign : int  2 4 1 3 1 3 4 2 1 1 ...</span>
<span class="c1">##  $ pdays    : int  999 999 999 999 999 999 999 999 999 999 ...</span>
<span class="c1">##  $ previous : int  0 0 0 0 0 2 0 0 1 0 ...</span>
<span class="c1">##  $ euribor3m: num  1.31 4.86 4.96 4.96 4.19 ...</span>
</pre></div>


<p>On this data, we've applied some predictive modeling techniques from the <a href="https://mlr-org.github.io/mlr/"><strong>mlr package</strong></a>. This popular R package is a wrapper for many predictive modeling techniques, such as logistic regression, random forest, XG boost, svm, neural nets and many, many others. For instance, to predict the binary target <strong>y</strong>, mlr currently offers the following algorithms:</p>
<div class="highlight"><pre><span></span><span class="c1"># To check available algorithms, create classification task with binary target y </span>
task <span class="o">=</span> mlr<span class="o">::</span>makeClassifTask<span class="p">(</span>data <span class="o">=</span> bank<span class="p">,</span> target <span class="o">=</span> <span class="s">&quot;y&quot;</span><span class="p">)</span>
algos <span class="o">=</span> mlr<span class="o">::</span>listLearners<span class="p">(</span>task<span class="p">,</span> check.packages <span class="o">=</span> <span class="kc">FALSE</span><span class="p">)</span><span class="o">$</span>name
<span class="kp">paste0</span><span class="p">(</span><span class="s">&#39;Available Algorithms (&#39;</span><span class="p">,</span><span class="kp">length</span><span class="p">(</span>algos<span class="p">),</span><span class="s">&#39;): &#39;</span><span class="p">,</span><span class="kp">paste</span><span class="p">(</span><span class="kp">head</span><span class="p">(</span>algos<span class="p">,</span><span class="m">5</span><span class="p">),</span>collapse <span class="o">=</span> <span class="s">&#39;, &#39;</span><span class="p">),</span><span class="s">&#39;, ..., &#39;</span><span class="p">,</span><span class="kp">paste</span><span class="p">(</span><span class="kp">tail</span><span class="p">(</span>algos<span class="p">,</span><span class="m">5</span><span class="p">),</span>collapse <span class="o">=</span> <span class="s">&#39;, &#39;</span><span class="p">))</span>
<span class="c1">## [1] &quot;Available Algorithms (80): C50, k-Nearest Neighbours, J48 Decision Trees, Propositional Rule Learner, L1-Regularized L2-Loss Support Vector Classification, ..., Deep neural network with weights initialized by Stacked AutoEncoder, Shrinkage Discriminant Analysis, Sparse Discriminant Analysis, Support Vector Machines (libsvm), eXtreme Gradient Boosting&quot;</span>
</pre></div>


<p><strong>mlr</strong> currently provides 80 different algorithms for binary classification, awesome, right? It should be noted, that to use our <strong>modelplotr</strong> package, you don't have to use <strong>mlr</strong> to build your models. More on this in the <strong>modelplotr</strong> package documentation, just have a look at <em>?modelplotr::aggregate_over_deciles()</em>. To have a few models to evaluate with our plots, we do take advantage of mlr's greatness.</p>
<div class="highlight"><pre><span></span><span class="c1"># prepare data for training and train models </span>
test_size <span class="o">=</span> <span class="m">0.3</span>
train_index <span class="o">=</span>  <span class="kp">sample</span><span class="p">(</span><span class="kp">seq</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> <span class="kp">nrow</span><span class="p">(</span>bank<span class="p">)),</span>size <span class="o">=</span> <span class="p">(</span><span class="m">1</span> <span class="o">-</span> test_size<span class="p">)</span><span class="o">*</span><span class="kp">nrow</span><span class="p">(</span>bank<span class="p">)</span> <span class="p">,</span>replace <span class="o">=</span> <span class="bp">F</span><span class="p">)</span>
train <span class="o">=</span> bank<span class="p">[</span>train_index<span class="p">,]</span>
test <span class="o">=</span> bank<span class="p">[</span><span class="o">-</span>train_index<span class="p">,]</span>

<span class="c1"># estimate models with mlr</span>
<span class="kn">library</span><span class="p">(</span>mlr<span class="p">)</span>
task <span class="o">=</span> makeClassifTask<span class="p">(</span>data <span class="o">=</span> train<span class="p">,</span> target <span class="o">=</span> <span class="s">&quot;y&quot;</span><span class="p">)</span>
lrn <span class="o">=</span> makeLearner<span class="p">(</span><span class="s">&quot;classif.randomForest&quot;</span><span class="p">,</span> predict.type <span class="o">=</span> <span class="s">&quot;prob&quot;</span><span class="p">)</span>
rf <span class="o">=</span> train<span class="p">(</span>lrn<span class="p">,</span> task<span class="p">)</span>
lrn <span class="o">=</span> makeLearner<span class="p">(</span><span class="s">&quot;classif.multinom&quot;</span><span class="p">,</span> predict.type <span class="o">=</span> <span class="s">&quot;prob&quot;</span><span class="p">)</span>
mnl <span class="o">=</span> train<span class="p">(</span>lrn<span class="p">,</span> task<span class="p">)</span>
lrn <span class="o">=</span> makeLearner<span class="p">(</span><span class="s">&quot;classif.xgboost&quot;</span><span class="p">,</span> predict.type <span class="o">=</span> <span class="s">&quot;prob&quot;</span><span class="p">)</span>
<span class="c1">## Error in requirePackages(package, why = stri_paste(&quot;learner&quot;, id, sep = &quot; &quot;), : For learner classif.xgboost please install the following packages: xgboost</span>
xgb <span class="o">=</span> train<span class="p">(</span>lrn<span class="p">,</span> task<span class="p">)</span>
lrn <span class="o">=</span> makeLearner<span class="p">(</span><span class="s">&quot;classif.lda&quot;</span><span class="p">,</span> predict.type <span class="o">=</span> <span class="s">&quot;prob&quot;</span><span class="p">)</span>
lda <span class="o">=</span> train<span class="p">(</span>lrn<span class="p">,</span> task<span class="p">)</span>
</pre></div>


<p>Ok, we've generated some predictive models. Let's prepare for plotting! First, we have to install our <strong>modelplotr</strong> package. Since it is available on github, it can easily be installed using <strong>devtools</strong>:</p>
<div class="highlight"><pre><span></span><span class="kn">library</span><span class="p">(</span>devtools<span class="p">)</span>
install_github<span class="p">(</span><span class="s">&quot;modelplot/modelplotr&quot;</span><span class="p">)</span>
</pre></div>


<p>As you'll probably see when you run it, it comes with a number of other very valuable package installs. In another post, we go into much more detail on our modelplot packages in r and python and all its functionalities. Here, we want to take you to the plots as soon as we can, so we plug and play: </p>
<div class="highlight"><pre><span></span><span class="kn">library</span><span class="p">(</span>modelplotr<span class="p">)</span>
<span class="c1">## Package modelplotr loaded! Happy model plotting!</span>

prepare_scores_and_deciles<span class="p">(</span>datasets<span class="o">=</span><span class="kt">list</span><span class="p">(</span><span class="s">&quot;train&quot;</span><span class="p">,</span><span class="s">&quot;test&quot;</span><span class="p">),</span>
  dataset_labels <span class="o">=</span> <span class="kt">list</span><span class="p">(</span><span class="s">&quot;train data&quot;</span><span class="p">,</span><span class="s">&quot;test data&quot;</span><span class="p">),</span>
  models <span class="o">=</span> <span class="kt">list</span><span class="p">(</span><span class="s">&quot;rf&quot;</span><span class="p">,</span><span class="s">&quot;mnl&quot;</span><span class="p">,</span><span class="s">&quot;xgb&quot;</span><span class="p">,</span><span class="s">&quot;lda&quot;</span><span class="p">),</span>
  model_labels <span class="o">=</span> <span class="kt">list</span><span class="p">(</span><span class="s">&quot;random forest&quot;</span><span class="p">,</span><span class="s">&quot;multinomial logit&quot;</span><span class="p">,</span><span class="s">&quot;XGBoost&quot;</span><span class="p">,</span><span class="s">&quot;Discriminant&quot;</span><span class="p">),</span>
  target_column<span class="o">=</span><span class="s">&quot;y&quot;</span><span class="p">)</span>
<span class="c1">## [1] &quot;Data preparation step 1 succeeded! Dataframe &#39;scores_and_deciles&#39; created.&quot;</span>

plotting_scope<span class="p">(</span>select_model_label <span class="o">=</span> <span class="s">&#39;XGBoost&#39;</span><span class="p">,</span>select_dataset_label <span class="o">=</span> <span class="s">&#39;test data&#39;</span><span class="p">)</span>
<span class="c1">## [1] &quot;deciles_aggregate not available; input_modelevalplots() is run...&quot;</span>
<span class="c1">## Data preparation step 3 succeeded! Dataframe &#39;plot_input&#39; created.</span>
<span class="c1">## </span>
<span class="c1">## No comparison specified! Single evaluation line will be plotted: </span>
<span class="c1">##  Target value &quot;yes&quot; plotted for dataset &quot;test data&quot; and model &quot;XGBoost.&quot;</span>
<span class="c1">##   To compare models, specify: scope = &quot;compare_models&quot;</span>
<span class="c1">##   To compare datasets, specify: scope = &quot;compare_datasets&quot;</span>
<span class="c1">##   To compare target classes, specify: scope = &quot;compare_targetclasses&quot;</span>
<span class="c1">##   To plot one line, do not specify scope or specify scope = &quot;no_comparison&quot;.</span>
</pre></div>


<p>What just happened? In the <strong>prepare_scores_and_deciles</strong> function, we've scored the customers in the train dataset and test dataset with their probability to acquire a term deposit, according to the predictive models we've just built with <strong>mlr</strong>. Aside from the datasets and model objects, we had to specify the name of the target variable and for our convenience, we gave our datasets and models some usefull labels. </p>
<p>In the second step, we specified the scope of the analysis. We've not specified the "scope" parameter, therefore the default - <em>no comparison</em> - is chosen. As the output notes, you can use modelplotr to evaluate your model(s) from several perspectives: </p>
<ul>
<li>Interpret just one model (the default)</li>
<li>Compare the model performance across different datasets</li>
<li>Compare the performance across different models</li>
<li>Compare the performance across different target values</li>
</ul>
<p>Here, we will keep it simple and evaluate - from a business perspective - how well a selected model will perform in a selected dataset for one target value. We did specify values for some parameters, to focus on the <strong>XGBoost</strong> model on the <strong>test data</strong>. The default value for the target class is <strong>yes</strong> ; since we want to focus on customers that do take term deposits, this default is perfect. </p>
<h3>Let’s introduce the Gains, Lift and (cumulative) Response Charts.</h3>
<p>Before we throw more code and output at you, let’s get you familiar with the plots we so strongly advocate to use to assess a predictive model’s business value. Although each plot sheds light on the business value of your model from a different angle, they all use the same data: </p>
<ul>
<li>Predicted probability for the target class, </li>
<li>Equally sized groups based on this predicted probability. </li>
<li>Actual number of observed target class observations in these groups.  </li>
</ul>
<p>It’s common practice to split the data to score into 10 equally large groups and call these groups deciles. Observations that belong to the top-10% with highest model probability in a set, are in decile 1 of that set; the next group of 10% with high model probability are decile 2 and finally the 10% observations with the lowest model probability on the target class belong to decile 10. </p>
<p>Each of our four plots places the deciles on the x axis and another measure on the y axis. The deciles are plotted from left to right so the observations with the highest model probability are on the left side of the plot. This results in plots like this:</p>
<p><img alt="''" src="img/decileplot.png" /> </p>
<p>Now that it’s clear what is on the horizontal axis of each of the plots, we can go into more detail on the metrics per plot on the vertical axis. Per plot, we’ll start with a brief explanation what insight you gain with the plot from a business perspective. After that, we apply it to our banking data and show some neat features of modelplotr to help you explain the value of your wonderful predictive models to others.</p>
<h4>The cumulative gains chart</h4>
<p>The cumulative gains chart - often named ‘gains chart’ - helps you answer the question:</p>
<p><strong><em><span style="color:orange">When we apply the model and select the best X deciles, what % of the actual target class observations can we expect to target?</span></em></strong> </p>
<p>Hence, the cumulative gains chart visualises the percentage of the target class members you have selected if you would decide to select up until decile X. This is a very important business question, because in most cases, you want to use a predictive model to target a <strong>subset</strong> of observations - customers, prospects, cases,... - instead of targeting all cases. And since we won't build perfect models all the time, we will miss some potential. And that's perfectly all right, because if we are not willing to accept that, we should not use a model in the first place. Or build a perfect model, that scores all actual target class members with a 100% probability and all the cases that do not belong to the target class with a 0% probability. However, if you’re such a wizard, you don’t need these plots any way or you should have a careful look at your model - maybe you’re cheating?....  </p>
<p>So, we'll have to accept we will loose some. <em>What percentage</em> of the actual target class members you do select with your model at a given decile, that’s what the cumulative gains chart tells you. The plot comes with two references to tell you how good/bad your model is doing: The <em>random model line</em> and the <em>wizard model line</em>. The random model line tells you what proportion of the actual target class you would expect to select when no model is used at all. This vertical line runs from the origin (with 0% of cases, you can only have 0% of the actual target class members) to the upper right corner (with 100% of the cases, you have 100% of the target class members). It’s the rock bottom of how your model can perform; are you close to this, then your model is not much better than a flip of a coin. The wizard model is the upper bound of what your model can do. It starts in the origin and rises as steep as possible towards 100%. If less than 10% of all cases belong to the target category, this means that it goes steep up from the origin to the value of decile 1 and cumulative gains of 100% and remains there for all other deciles as it is a cumulative measure. Your model will always move between these two reference lines - closer to a wizard is always better -  and looks like this: </p>
<p><img alt="''" src="img/cumgainsplot.png" /></p>
<p>Back to our business example. How many of the term deposit buyers can we select with the top-20% of our predictive models? Let's find out! To generate the cumulate gains plot, we can simply call the function <strong>plot_cumgains()</strong>:</p>
<div class="highlight"><pre><span></span>plot_cumgains<span class="p">()</span>
</pre></div>


<p><img alt="plot of chunk gainsplot" src="https://modelplot.github.io/img/IntroModelplotr-gainsplot-1.png" /></p>
<p>We don't need to specify any parameters, since the default input is <strong>plot_input</strong>, which is generated with the <strong>plotting_scope()</strong> function we ran previously. There are several parameters available to customize the plot, though. If we want to emphasize the model performance at a given point, we can add both highlighting to the plot and add some explanatoty text below the plot. Both are optional, though:</p>
<div class="highlight"><pre><span></span>plot_cumgains<span class="p">(</span>highlight_decile <span class="o">=</span> <span class="m">2</span><span class="p">)</span>
<span class="c1">##  </span>
<span class="c1">## Plot annotation:</span>
<span class="c1">## - When we select 20% with the highest probability according to XGBoost, this selection holds 78% of all yes cases in test data.</span>
<span class="c1">##  </span>
<span class="c1">## </span>
</pre></div>


<p><img alt="plot of chunk gainsplotannotated" src="https://modelplot.github.io/img/IntroModelplotr-gainsplotannotated-1.png" /></p>
<p>Our 'highlight_decile' parameter ads some guiding elements to the plot at decile 2 as well as a text box below the graph with the interpretation of the plot at decile 2 in words. This interpretation also printed to the console. Our simple model - only 6 pedictors were used - seems to do a nice job selecting the customers interested in buying term deposites. <em>When we select 20% with the highest probability according to XGBoost, this selection holds 80% of all yes cases in test data.</em> With a perfect model, we would have selected 100%, since less than 20% of all customers in the test set buy term deposits.A random pick would only hold 20% of the customers with term deposits. How much better than random we do, brings us to plot number 2! </p>
<h4>The cumulative lift chart</h4>
<p>The cumulative lift chart, often referred to as lift chart or index chart, helps you answer the question:</p>
<p><strong><em><span style="color:orange">When we apply the model and select the best X deciles, how many times better is that than using no model at all?</span></em></strong></p>
<p>The lift chart helps you in explaining how much better selecting based on you model is compared to taking random selections instead. Especially when models are not yet used within a certain organisation or domain, this really helps business understand what selecting based on models can do for them. </p>
<p>The lift chart only has one reference line: the ‘random model’.  With a random model we mean that each observation gets a random number and all cases are devided into deciles based on these random numbers. The % of actual target category observations in each decile would be equal to the overall % of actual target category observations in the total set. Since the lift is calculated as the ratio of these two numbers, we get a flat line at the value of 1. Your model should however be able to do better, resulting in a high ratio for decile 1. How high the lift can get, depends on your model' quality 's predictive power, but also on the % of target class observations in the data: If 50% of your data belongs to the target class of interest, a perfect model would only do twice as good (lift: 2) as a random selection. With a smaller target class value, say 10%, the model can potentially be 10 times better (lift: 10) than a random selection. Therefore, no general guideline of a 'good' lift can be specified. Towards decile 10, since the plot is cumulative, with 100% of cases, we have the whole set again and therefore the cumulative lift will always end up at a value of 1. It looks like this:</p>
<p><img alt="''" src="img/cumliftplot.png" /></p>
<p>To generate the cumulative lift plot for our XGBoost model predicting term deposit buying, we call the function <strong>plot_cumlift()</strong>. Let's add some highlighting to see how much better a selection based on our model containing deciles 1 and 2 would be, compared to a random selection of 20% of all customers:</p>
<div class="highlight"><pre><span></span>plot_cumlift<span class="p">(</span>highlight_decile <span class="o">=</span> <span class="m">2</span><span class="p">)</span>
<span class="c1">##  </span>
<span class="c1">## Plot annotation:</span>
<span class="c1">## - When we select 20% with the highest probability according to model XGBoost in test data, this selection for yes cases is 3.9 times better than selecting without a model.</span>
<span class="c1">##  </span>
<span class="c1">## </span>
</pre></div>


<p><img alt="plot of chunk liftplot" src="https://modelplot.github.io/img/IntroModelplotr-liftplot-1.png" /></p>
<p>A term deposit campaign targeted at a selection of 20% of all customers based on our XGBoost model can be expected to have a 4 times higher response (400%) compared to a random sample of customer. Not bad, right? The cumulative lift really helps in getting a positive return on marketing investments. It should be noted, though, that since the cumulative lift plot is relative, it doesn't tell us how high the actual reponse will be on our campaign...   </p>
<h4>The response plot</h4>
<p>One of the easiest to explain evaluation plots is the response plot. It simply plots the percentage of target class observations per decile. It can be used to answer the following business question:</p>
<p><strong><em><span style="color:orange">When we apply the model and select decile X, what is the expected % of target class observations in that decile?</span></em></strong> </p>
<p>The plot has one reference line: The % of target class cases in the total set. It looks like this:</p>
<p><img alt="''" src="img/responseplot.png" /></p>
<p>A good model starts with a high response value in the first decile(s) and suddenly drops quickly towards 0 for later deciles. This indicates good differentiation between target class members - getting high model scores - and all other cases. An interesting point in the plot is the location where your model’s line intersects the random model line. From that decile onwards, the % of target class cases is lower than a random selection of cases would hold.  </p>
<p>To generate the response plot for our term deposit model, we can simply call the function <strong>plot_response()</strong>. Let's immediately highlight the plot to have the interpretation of the response plot at decile 1 added to the plot:</p>
<div class="highlight"><pre><span></span>plot_response<span class="p">(</span>highlight_decile <span class="o">=</span> <span class="m">1</span><span class="p">)</span>
<span class="c1">##  </span>
<span class="c1">## Plot annotation:</span>
<span class="c1">## - When we select decile 1 according to model XGBoost in dataset test data the % of yes cases in the selection is 56%.</span>
<span class="c1">##  </span>
<span class="c1">## </span>
</pre></div>


<p><img alt="plot of chunk responseplot" src="https://modelplot.github.io/img/IntroModelplotr-responseplot-1.png" /></p>
<p>As the plot shows and the text below the plot states: <em>When we select decile 1 according to model XGBoost in dataset test data the % of yes cases in the selection is 55%.</em>. This is quite good, especially when compared to the overall likelihood of 11%. The response in the second decile is much lower, about 28%. From decile 3 onwards, the expected response will be lower than the overall likelihood of 10.4%. However, most of the time, our model will be used to select the highest decile <em>up until</em> some decile. That makes it even more relevant to have a look at the cumulative version of the response chart. And guess what, that's our final plot!   </p>
<h4>The cumulative response plot</h4>
<p>Finally, one of the most used charts: The cumulative response chart. It answers the question burning on each business reps lips:</p>
<p><strong><em><span style="color:orange">When we apply the model and select up until decile X, what is the expected % of target class observations in the selection?</span></em></strong></p>
<p>The reference line in this plot is the same as in the response chart: the % of target class cases in the total set. </p>
<p><img alt="''" src="img/cumresponseplot.png" /></p>
<p>Whereas the response plot crosses the reference line, in the cumulative response plot never crosses it but ends up at the same point for decile 10: Selecting all cases up until decile 10 is the same as selecting all cases, hence the % of target class cases will be exactly the same.This plot is most often used to decide - together with business colleagues - up until what decile to select for a campaign based upon the model. </p>
<p>Back to our banking business case. To generate the cumulative response plot, we call the function <strong>plot_cumresponse()</strong>. Let's highlight it at decile 3 to see what the overall expected response will be if we select prospects for a term deposit offer based on our XGBoost model:</p>
<div class="highlight"><pre><span></span>plot_cumresponse<span class="p">(</span>highlight_decile <span class="o">=</span> <span class="m">3</span><span class="p">)</span>
<span class="c1">##  </span>
<span class="c1">## Plot annotation:</span>
<span class="c1">## - When we select deciles 1 until 3 according to model XGBoost in dataset test data the % of yes cases in the selection is 33%.</span>
<span class="c1">##  </span>
<span class="c1">## </span>
</pre></div>


<p><img alt="plot of chunk cumresponseplot" src="https://modelplot.github.io/img/IntroModelplotr-cumresponseplot-1.png" /></p>
<p><em>When we select deciles 1 until 3 according to model XGBoost in dataset test data the % of yes cases in the selection is 36%.</em> Since the test data is an independent set, not used to train the model, we can expect the response on the term deposit campaign to be 36%. </p>
<p>The cumulative response percentage at a given decile is a number your business colleagues can really work with: Is that response big enough to have a successfull campaign, given costs and other expectations? Will the absolute number of sold term deposits meet the targets? Or do we loose too much of all potential term deposit buyers by only selecting the top 30%? To answer that question, we can go back to the cumulative gains plot. And that's why there's no absolute winner among these plots and we advice to use them all. To make that happen, there's also a function to easily combine all four plots. </p>
<h4>All four plots together</h4>
<p>With the function call <strong>plot_all</strong> we get all four plots on one grid. We can easily save it to a file to include it in a presentation or share it with colleagues.</p>
<div class="highlight"><pre><span></span>plot_all<span class="p">(</span>save_fig <span class="o">=</span> <span class="kc">TRUE</span><span class="p">,</span>save_fig_filename <span class="o">=</span> <span class="s">&#39;Selection model Term Deposits&#39;</span><span class="p">)</span>
<span class="c1">## Plot is saved as: c:/TEMP/jupyter-blog/content/Selection model Term Deposits.png</span>
</pre></div>


<p><img alt="plot of chunk allplots" src="https://modelplot.github.io/img/IntroModelplotr-allplots-1.png" /></p>
<p>Neat! With these plots, we are able to talk with business people about the actual value of our predictive model, without having to bore them with technicalities any nitty gritty details. We've translated our model in business terms and visualised it to ease interpretation. Hopefully, this helps many of you in discussing how to optimally take advantage of your predictive model building efforts.   </p>
<h3>Three other scopes in modelplotr.</h3>
<p>As briefly discussed earlier, the modelplotr package doesn't only allow to add a single line to the plots, but also enable to make interesting comparisons. Comparisons between different models, between different datasets and (in case of a multiclass target) between different target classes. To discuss all this, please have a look at the package documentation or read our other posts on modelplotr. </p>
<p>However, to give one example, we could compare whether XGBoost was indeed the best choice to select the top-30% customers for a term deposit offer:  </p>
<div class="highlight"><pre><span></span>plotting_scope<span class="p">(</span>scope <span class="o">=</span> <span class="s">&quot;compare_models&quot;</span><span class="p">)</span>
<span class="c1">## Data preparation step 3 succeeded! Dataframe &#39;plot_input&#39; created.</span>
<span class="c1">## </span>
<span class="c1">## Models &quot;random forest&quot;, &quot;multinomial logit&quot;, &quot;XGBoost&quot;, &quot;Discriminant&quot; compared for dataset &quot;test data&quot; and target value &quot;yes&quot;.</span>

plot_cumresponse<span class="p">(</span>highlight_decile <span class="o">=</span> <span class="m">3</span><span class="p">,</span>highlight_how <span class="o">=</span> <span class="s">&quot;plot_text&quot;</span><span class="p">)</span>
<span class="c1">##  </span>
<span class="c1">## Plot annotation:</span>
<span class="c1">## - When we select deciles 1 until 3 according to model random forest in dataset test data the % of yes cases in the selection is 32%.</span>
<span class="c1">## - When we select deciles 1 until 3 according to model multinomial logit in dataset test data the % of yes cases in the selection is 33%.</span>
<span class="c1">## - When we select deciles 1 until 3 according to model XGBoost in dataset test data the % of yes cases in the selection is 33%.</span>
<span class="c1">## - When we select deciles 1 until 3 according to model Discriminant in dataset test data the % of yes cases in the selection is 32%.</span>
<span class="c1">##  </span>
<span class="c1">## </span>
</pre></div>


<p><img alt="plot of chunk comparemodels" src="https://modelplot.github.io/img/IntroModelplotr-comparemodels-1.png" /></p>
<p>Seems like the algorithm used will not make a big difference in this case. Hopefully you agree by now that using these plots really can make a difference in explaining the business value of your predictive models! Happy modelplotting!</p>
      </div>
<!-- /.entry-content -->
      <footer class="post-info text-muted">
        <button type="button" class="btn btn-default">          
          <a href="https://modelplot.github.io/category/misc.html"><div class="fa fa-lg fa-folder-open"></div> misc</a>
        </button>
      </footer>
      <!-- /.post-info -->
    </section>
    </div>
    <footer class="footer">
      <div class="container">
        <p class="footer-text">&copy; <a href="https://modelplot.github.io">intro modelplot</a> powered by <a href="http://getpelican.com/">pelican</a> and <a href="http://nodotcom.org">nikhil</a></p>
      </div>
    </footer>
    <script src="//ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
    <script src="//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
  </body>
</html>